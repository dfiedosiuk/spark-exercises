package P03

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions.{count, countDistinct}

object P03 extends App {

  val spark = SparkSession
    .builder()
    .master("local[*]")
    .getOrCreate()

  import spark.implicits._

  val input = Seq(
    ("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001",  2),
    ("05:49:56.604900", "10.0.0.2.54880", "10.0.0.3.5001",  2),
    ("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001",  2),
    ("05:49:56.604900", "10.0.0.2.54880", "10.0.0.3.5001",  2),
    ("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001",  2),
    ("05:49:56.604900", "10.0.0.2.54880", "10.0.0.3.5001",  2),
    ("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001",  2),
    ("05:49:56.604900", "10.0.0.2.54880", "10.0.0.3.5001",  2),
    ("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001",  2),
    ("05:49:56.604900", "10.0.0.2.54880", "10.0.0.3.5001",  2),
    ("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001",  2),
    ("05:49:56.604900", "10.0.0.2.54880", "10.0.0.3.5001",  2),
    ("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001",  2),
    ("05:49:56.604908", "10.0.0.3.5001",  "10.0.0.2.54880", 2),
    ("05:49:56.604908", "10.0.0.3.5001",  "10.0.0.2.54880", 2),
    ("05:49:56.604908", "10.0.0.3.5001",  "10.0.0.2.54880", 2),
    ("05:49:56.604908", "10.0.0.3.5001",  "10.0.0.2.54880", 2),
    ("05:49:56.604908", "10.0.0.3.5001",  "10.0.0.2.54880", 2),
    ("05:49:56.604908", "10.0.0.3.5001",  "10.0.0.2.54880", 2),
    ("05:49:56.604908", "10.0.0.3.5001",  "10.0.0.2.54880", 2)).toDF("column0", "column1", "column2", "label")

  val columns1and2 = Window.partitionBy("column1", "column2")

  val counts = input.withColumn("count", count($"label") over columns1and2)

}
